# Spark

## Recap
Spark is built around the concepts of Resilient Distributed Datasets and Direct Acyclic Graph representing transformations and dependencies between them.

![](Spark-Overview--1-.png)

Spark Application (often referred to as Driver Program or Application Master) at high level consists of SparkContext and user code which interacts with it creating RDDs and performing series of transformations to achieve final result. These transformations of RDDs are then translated into DAG and submitted to Scheduler to be executed on set of worker nodes.

## RDD: Resilient Distributed Dataset

- **Resilient** means that we must be able to withstand failures and complete an ongoing computation. 
- **Distributed** means that we must account for multiple machines having a subset of data. 

**Formally, RDD is a read-only, distributed collection of objects. These objects are grouped into partitions (partitioned collection) which are stored and processed by executors.**

Spark represents the computation in the so-called lineage graph. It consists of a sequence of RDDs which are generated by the transformations and the dependencies between the RDDs.

RDD represents distributed immutable data (partitioned data + iterator) and lazily evaluated operations (transformations). As an interface RDD defines five main properties:
```Scala
//a list of partitions (e.g. splits in Hadoop)
def getPartitions: Array[Partition]

//a list of dependencies on other RDDs
def getDependencies: Seq[Dependency[_]]

//a function for computing each split
def compute(split: Partition, context: TaskContext): Iterator[T]

//(optional) a list of preferred locations to compute each split on
def getPreferredLocations(split: Partition): Seq[String] = Nil

//(optional) a partitioner for key-value RDDs
val partitioner: Option[Partitioner] = None  
```

Here's an example of RDDs created during a call of method sparkContext.textFile("hdfs://...") which first loads HDFS blocks in memory and then applies map() function to filter out keys creating two RDDs:

![](DAG-logical-vs-partitions-view--3-.png)

HadoopRDD:
    * getPartitions = HDFS blocks
    * getDependencies = None
    * compute = load block in memory
    * getPrefferedLocations = HDFS block locations
    * partitioner = None
MapPartitionsRDD
    * getPartitions = same as parent
    * getDependencies = parent RDD
    * compute = compute parent and apply map()
    * getPrefferedLocations = same as parent
    * partitioner = None

Datasets must be typed
You can not modify data in-place in Spark. Datasets are immutable


**Partitions**
**Dependencies** (that models the relationships a RDD and its partitions and the partition which it was derived from)
**Function:** for comping the dataset based on its parent RDD
**Metadata** about its partitioning scheme and data placement


**a binary file in HDFS**

**partitions() -> Array[Partition]**
• lookup blocks information from the NameNode
• make a partition for every block
› return an array of the partitions
**iterator(p: Partition, parents: Array[Iterator[_]]) -> Iterator[Byte]**
• parents are not used
• return a reader for the block of the given partition
**dependencies() -> Array[Dependency]**

**Example: an sliced* in-memory array**
*can be used to parallelize in-memory computations

**partitions() -> Array[Partition]**
› slice array in chunks of size N
›make a partition for every chunk
› return an array of a single partition with the source array of the
partitions
**iterator(p: Partition, parents: Array[Iterator[_]]) -> Iterator[T]**
›parents are not used
›return an iterator over the source array chunk in the given partition
**dependencies() -> Array[Dependency]**
›return an empty array (no dependencies)


#### Prtitioner
The partitioner defines how records will be distributed and thus which records will be completed by each task

Interface with 2 methods:

* `numPartitions` – number of partitions in RDD after partitioning
    ```python
    groupByKey(numPartitions=None,
        partitionFunc=<function portable_hash…>)
    ```
    Group the values for each key in the RDD into a single sequence. Hash-partitions the resulting RDD with numPartitions partitions.

* `getPartition` – mapping from a key to the index of partition

**Spark partitioners**
* hash partitioner
* range partitioner (e.g. sorting) Range partitioning assigns records, whose keys are in the same region to the given partition. 

**Transformations preserving partitioning**
> Manual transformations have a preserves partitioning argument which is set to false by default. It becomes obvious for the map method for example, because map might spill not only new values but new keys. But if you are 100% sure that your keys don't modify, you can set preservesPartitioning to true, or use map values which preserves a partitioner. 
* map(f, preservesPartitioning=False)
* flatMap(f, preservesPartitioning=False)
* mapPartitions(f, preservesPartitioning=False)  iterator-to-iterator transformation
* mapPartitionsWithIndex(f, preservesPartitioning=False)
* mapValues(f)
* flatMapValues(f)

* RDDs are *co-partitioned* if they are partitioned by the same known partitioner -> Co-partitioned RDDs reduce the volume of the shuffle
* Partitions are *co-located* if they are both loaded into memory on the same machine -> Co-located RDDs don’t shuffle
* Default partitioner uses key hashing

#### Execurors

![](execurors_0.png)

An executor in Spark is just a JVM holding a bunch of objects in its memory.  Spark has to do before mini shuffle, is having an object one in executor one, transfer it through the network through the JVM of the second executor.

Spark uses serialization.
> Serialization is the process of translating data structures or object state into a format that can be stored (for example, in a file or memory buffer, or transmitted across a network connection link) and reconstructed later in the same or another computer environment

Spark serializers:
* Java – slow, but robust
* Kryo – fast, but has corner cases

![](execurors_1.png)

PySpark context is a Python interpreter which communicates with the JVM using this socket, and Py4J library. 
Py4J provides mechanism of calling Gela methods and wrapping objects. 
On Spark workers, Python is just a protest which starts side by side with the JVM and communicates through Unix pipes. 

![](execurors_2.png)

At first Spark has to serialize Python objects and pass them through JVM, where they should be serialized again for Spark's execution model to work.


### RDD Operations
Operations on RDDs are divided into several groups:
> Main - transformtions nd actions

#### Transformations
    * RDD input, RDD output
    * apply user function to every element in a partition (or to the whole partition)
    * apply aggregation function to the whole dataset (groupBy, sortBy)
    * introduce dependencies between RDDs to form DAG
    * provide functionality for repartitioning (repartition, partitionBy)
    * filter records and group them by a key
    * create new RDDs from existing RDDs by specifying how to obtain new items from the existing items
    * all transformtions are lazy
    Transformation is the primary way to “modify” data (given that RDDs are immutable)
    * There are transformations with **narrow**(local) and **wide** **dependencies**(require data shuffling). The transformation creates a new RDD every time, so you can't produce a cyclic dependency graph by applying Spark transformations to RDDs
    * MapReduce can be expressed with a couple of transformations
    * Complex transformations (like joins, cogroup) are available

> Narrow and Wide dependencies
>
> Conceptually, narrow transformations are those in which each partition in the child RDD has simple finite dependencies on partitions in the parent RDD. Dependencies are only narrow if they can be determined at their designed time, irrespective of the values of the records in the parent partitions and if each parent has at most one child partition. 
>  Specifically, partitions in narrow transformations can either depend on one parent, such as in the map operator, or a unique subset of the parent partitions that is known at designed time, like call ask. Thus, narrow transformations can be executed on an arbitrary subset of the data without any information about other partitions. 
>  join may be a narrow transformation if the partitions of the RDD are co-partitioned.
>
> wide dependencies cannot be executed on arbitrary rows and instead, require the data to be partitioned in a particular way. Transformations with wide dependencies include sort, reduceByKey, groupByKey, join, and anything that cause the repartition function.
> Wide dependencies, which cause data shuffling, create new stages.
>
> The division into narrow and wide dependencies creates stages of execution. This means that the sub-sequence of a lineage graph which has only narrow dependencies can be computed without any data transfer in a nice pipelined fashion. 
> Wide dependencies cause data transfers which are called shuffles. Shuffles are expensive, so they should be avoided or the volume of the transfer data should be reduced.


it's possible to check if each keyA exists in B dataset, B in the memory on Map phase
each mapper doesn't store all the keys from A dataset. So you can't check if keyB exists in A

Freq used
› Def: filter(p: T  Boolean): RDD[T]  RDD[T]
    ›returns a filtered RDD with items satisfying the predicate p
› Def: map(f: T  U): RDD[T]  RDD[U]
    ›returns a mapped RDD with items f(x) for every x in the source RDD
› Def: flatMap(f: T  Array[U]): RDD[T]  RDD[U]
    › same as map but flattens the result of f
    ›generalizes map and filter

**Filter**

› Y = X.filter(p) # where X : RDD[T]
    ›Y.partitions()  Array[Partition]
        › return the same partitions as X  
    ›Y.iterator(p: Partition, parents: Array[Iterator[T]])  Iterator[T]
        › take a parent iterator over the corresponding partition of X
        › wrap the parent iterator to skip items that do not satisfy the predicate
        › return the iterator over partition of Y
    ›Y.dependencies()  Array[Dependency]
        › k-th partition of Y depends on k-th partition of X

On closures
› Y = X.filter(lambda x: x % 2 == 0)
    ›predicate closure is captured within the Y (it is a part of the definition of Y)
    ›predicate is not guaranteed to execute locally (closure may be sent over the network to the executor)

**Partition dependency graph**

 Z = X.filter(lambda x: x % 2 == 0).filter(lambda y: y < 3)
![](0_kAw8hogu1oZPy9QU.png)

Something that is not that obvious in the MapReduce paradigm is the **cogroup transformation**. Unlike the previous transformations, cogroup operates on two keyed inputs. The result is the keyed output with the value being a pair of arrays holding the values collected for the given key from the first and the second input.
Joins, GroupBy - shuffle



#### Actions
    * trigger job execution. So Actions, together with a transformation code, are executed elsewhere, not in your driver program. Your driver program receives only the outcome.
    * used to materialize computation results - collect, print, save, and fold data.
    
    * When running locally, the executors are collocated within the same process as the driver program.
    * When running in a cluster mode, the executors are located on the cluster machines, thus allowing you to use the cluster for a computation.

Freq used ations: 
* collect()
    › collects items and passes them to the driver
    › for small datasets! all data is loaded to the driver memory
* take(n: Int)
    › collects only n items and passes them to the driver
    ›tries to decrease amount of computation by peeking on partitions
* top(n: Int)
    › collects n largest items and passes them to the driver
* reduce(f: (T, T)  T)
    › reduces all elements of the dataset with the given associate, commutative binary function and passes the result back to the driver
* saveAsTextFile(path: String)
    › each executor saves its partition to a file under the given path with every item converted to a string and confirms to the driver
* saveAsHadoopFile(path: String, outputFormatClass: String)
    ›each executor saves its partition to a file under the given path using the given Hadoop file format and confirms to the driver
* foreach(f: T  ())
    ›each executor invokes f over every item and confirms to the driver
* foreachPartition(f: Iterator[T]  ())
    ›each executor invokes f over its partition and confirms to the driver

#### Extra: persistence
    * explicitly store RDDs in memory, on disk or off-heap (cache, persist)
    * checkpointing for truncating RDD lineage

### Resiliency

#### Fault-tolerance in MapReduce
› Two key aspects
    ›reliable storage for input and output data
    ›deterministic and side-effect free execution of mappers and reducers

#### Fault-tolerance in Spark

› Same two key aspects
    ›reliable storage for input and output data
    ›deterministic and side-effect free execution of transformations(including closures)

› Determinism — every invocation of the function results in the same
returned value
    ›e. g. do not use random numbers, do not depend on a hash value order

› Freedom of side-effects — an invocation of the function does not change anything in the external world
    ›e. g. do not commit to a database, do not rely on global variables

#### Fault-tolerance & transformations
› Lineage — a dependency graph for all partitions of all RDDs involved in a computation up to the data source

if the dependencies of a failed partition fails as well then Computation is restarted to recompute the dependencies first, and the partition afterwards.



#### Fault-tolerance & actions
› Actions are side-effects in Spark
› Actions have to be idempotent that is safe to be re-executed multiple times given the same input

## Jobs, stages, tasks

› The SparkContext is the core of your application
› The driver communicates directly with the executors
› Execution goes as follows: Action -> Job -> Job Stages -> Tasks
› Transformations with narrow dependencies allow pipelining


**Job stage** is a pipelined computation spanning between materialization boundaries
    › not immediately executable
    › Job is spawned in response to a Spark action
    › Job is divided in smaller sets of tasks called stages
**Task** is a job stage bound to particular partitions
    › immediately executable
    › Task is a unit of work to be done
    › Tasks are created by a job scheduler for every job stage
Materialization happens when reading, shuffling or passing data to an action
    ›narrow dependencies allow pipelining
    ›wide dependencies forbid it

Example:
1. Invoking an action…
2. …spawns the job…
3. …that gets divided into
the stages by the job scheduler…
4. …and tasks are created for every job stage.

### SparkContext – other functions

› Tracks liveness of the executors
    ›required to provide fault-tolerance
› Schedules multiple concurrent jobs
    ›to control the resource allocation within the application
› Performs dynamic resource allocation
    ›to control the resource allocation between different applications

How does your application find out the executors to work with?
The SparkContext object allocates the executors by communicating with the cluster manager.

## Caching & Persistence

Persist - method to actually store intermediate results of a computation. It computes the whole lineage graph and then stores partitions of an RDD or dataframe in some place. 
And actually, when you call an action on a persistent data, nothing happens except computing this only action. And if you call collect, Spark doesn't have to recompute the whole thing, it just transfers data to your driver program. If that persistent is not a reliable method to store results but it is robust. 
So, if you have a dataframe consisting of several partitions on different executors, if one or more executors fail and are restarted by the resource manager, Spark will recompute lost partitions but will not recompute those which are in order.

Performance may be improved by persisting data across operations
    ›in interactive sessions, iterative computations and hot datasets
You can control the persistence of a dataset
    ›whether to store in the memory or on the disk
    ›how many replicas to create

Cache method 

Controlling persistence level
› rdd.persist(storageLevel)
    › sets RDD’s storage to persist across operations after it is computed for the first time
› storageLevel is a set of flags controlling the persistence, typical values are
    1lv MEMORY_ONLY
        – keep the data in the memory. 
        - executor has some reserved memory for persisting and when you choose MEMORY_ONLY, Spark tries to save partitions in this part of memory if there is enough space. 
    1lv MEMORY_ONLY_2
        – keep the data in the memory
        - provides a bit of redundancy by storing two replicas on different cluster nodes.
    2lv MEMORY_AND_DISK
        – keep the data in the memory; when out of memory – save it to the disk
        - Spark tries to store as much of data as possible in memory and if there is not enough space, it will spill records to disk. 
        - IO
    3lv DISK_ONLY
        – save the data to the disk, and doesn't store it in memory.
    DISK_ONLY_2, MEMORY_ONLY_2, MEMORY_AND_DISK_2
        – same as about, but make two replicas <- improves failure recovery times!
› rdd.cache() = rdd.persist(MEMORY_ONLY)

**checkpoints**
`checkpoint()` - as persistelce provides ways to store intermediate results. Spark writes partitions of an RDD or a dataframe to a reliable storage and this storage is HDFS.
*It turnkeys the whole lineage graph which produced this intermediate result.*
You need to specify a directory in HDFS where to store the data and then call checkpoint method of an RDD 
```python
sc.setCheckpointDir("/hdfs_directory/")
df.rdd.checkpoint()
```

When to make a checkpoint:
* Noisy cluster (Cluster with a lots of jobs and users which compete for resources and there are not enough resources to run all the jobs simultaneously.)
* Expensive and long computations
* No way to checkpoint Dataframe so you have to checkpoint the underlying RDD.

Best practices
› For interactive sessions
    › cache preprocessed data
› For batch computations
    › cache dictionaries
    › cache other datasets that are accessed multiple times
› For iterative computations
    › cache static data
› And do benchmarks!

Tips for persist and checkpoins
* MEMORY_ONLY if it fits
* Recomputing may be as fast as reading from disk
* Persist iterative algorithms (ML)
* Persisting is unreliable, checkpointing is reliable
* Checkpointing is slow, persisting may be slow

## Broadcast variables

› Broadcast variable is a read-only variable that is efficiently shared among tasks
› Distribution is done by a torrent-like protocol (extremely fast!)
› Distributed efficiently compared to captured variables

› Broadcast variables are read-only shared variables with effective sharing mechanism
› Useful to share dictionaries, models

Example
```python
sc = SparkContext(conf=…)
# compute the dictionary
my_dict_rdd = sc.textFile(…).map(…).filter(…)
my_dict_data = my_dict_rdd.collect()
# distributed the dictionary via the broadcast variable
broadcast_var = sc.broadcast(my_dict_data)
# use the broadcast variable within the task
my_data_rdd = sc.textFile(…).filter(
    lambda x: x in broadcast_var.value)
```

## Accumulator variables

› Accumulator variable is a read-write variable that is shared among tasks
› Writes are restricted to increments!
    ›increments only
    ›i. e.: var += delta 
    ›addition may be replaced by any associate, commutative operation
› can use custom associative, commutative operation for the updates
› Reads are allowed only by the driver program!
› can read the total value only in the driver
› Useful for the control flow, monitoring, profiling & debugging

When the accumulator gets incremented on the particular executor, the delta is sent back to the driver program together with a task outcome. Therefore, there is just one natural aggregation point where we can compute the total value, that is the driver. 
It is also important to note that updates generated in actions are guaranteed to be applied only once to the accumulator. This is because successful actions are never re-executed and Spark can conditionally apply the update.

For updates in transformation, there are no guarantees when they accumulate updates. Transformations can be recomputed on a failure on the memory pressure or in another unspecified codes like a preemption. Spark provides no guarantees on how many times transformation code maybe re-executed.

Use cases
› Performance counters
    ›# of processed records, total elapsed time, total error and so on and so forth
› Simple control flow
› conditionals: stop on reaching a threshold for corrupted records
    ›loops: decide whether to run the next iteration of an algorithm or not
› Monitoring
    ›export values to the monitoring system
› Profiling & debugging

# Spark Execution Model & RDD Internals

Spark application optimization.

Read the log file, filter out rows that contain info substring, and finally, a typical MapReduce pattern to emit one for each key and reduceByKey.
```python
logs = sc.textFile("log.txt")
    .filter(lambda x: "INFO" not in x)
    .map(lambda x: (x.split("\t")[1], 1))
    .reduceByKey(lambda x, y: x + y)
logs.collect()
```

**Shuffle Write Size** - instrumentation mechanism to spot the shuffle, and to measure how much data it has transferred.
Check UI and DUG interface to check how its going

Preserve the partitioner to avoid shuffles

PySpark adds double serialization
Try to reduce serialization overhead by pipelining operations inside the Python interpreter. 
PySpark tries to reduce serialization by pipelining. This produces strange DAGs

Don’t initialize objects inside your functions

Broadcast variables to reduce initialization overhead
```python
rdd.mapPartitions(myfunc)
obj = sc.broadcast(SomeLongRunningInit())
def myfunc(row_iter):
    for row in row_iter:
        yield obj.value.apply(row)
```

Transform partitions, not rows

joins generaed shuffles

* Reduce shuffles!
* Explicit (known) partitioner
* Preserve partitioner
* Reduce shuffle volume
    * reduceByKey vs groupByKey

### Memory

Two kinds of memory:
* Execution
    **Shuffles, joins, sorts and aggregations**
* Storage
    **Cache data**

* If execution memory is full – spill to disk
* If storage memory is full – evict LRU blocks
* It is better to evict storage, not execution
* If your app relies on caching, tune `spark.memory.storageFraction`

![](memory.png)

`spark.executor.memory = 1g` - Amount of memory to use per executor process
`spark.memory.fraction = 0.6` - Fraction of heap space used for execution and storage
`spark.memory.storageFraction = 0.5` - Amount of storage memory immune to eviction

**Memory management strategy**
* Evict storage, not execution!
* Minimum unevicatable amount of cached data

### Resource Allocation

What should I do?
```
pyspark --num_executors ??
        --executor-cores ??
        --executor-memory ??
```

* Least granular executors can’t use JVM parallelism
* Need to leave resources for OS and YARN daemons
* Need to leave resources for AM
* HDFS throughput is bounded by 5 cores
* Need to to leave resources for off-heap

Resource managers
* Spark standalone
* Mesos
* *YARN*

HDFS Throughput
* 15 cores per executor is bad for HDFS throughput
* Rule of thumb – 5 cores per executor

Final try
* 1 node = 16 - 1 cores, 64 - 1 G
* 1 executor = 5 cores (best HDFS throughput)
* 60 cores / 5 cores per executor = 12 executors
* 1 container for Appllication Master
* 63G / 3 executors per node = 21 * 0.9 (off-heap) = 19G
pyspark --num_executors 11
 --executor-cores 5
 --executor-memory 19G

### Dynamic Allocation

> https://www.slideshare.net/databricks/dynamic-allocation-in-spark
> https://www.slideshare.net/SparkSummit/spark-summit-eu-talk-by-luc-bourlier

* Spark uses long-running containers for speed
* This may cause underutilization
* Dynamic allocation starts executors when needed and releases when not needed
* Dynamic allocation requires tuning
* Shuffle results should be stored in an external service

MR uses short-lived containers for each task
Spark reuses long-running containers for speed

use cases
* Long-running ETL jobs
* Interactive application (Jupyter Notebook)
* Applications with large shuffles


`spark.dynamicAllocation.enabled = false` - Whether to use dynamic resource allocation
`spark.dynamicAllocation.executorIdleTimeout = 60s` -  If an executor has been idle for more than this duration, the executor will be removed
`spark.dynamicAllocation.cachedExecutorIdleTimeout = infinity` - If an executor which has cached data blocks has been idle for more than this duration, the executor will be removed
`spark.dynamicAllocation.minExecutors = 0` - Lower bound for the number of executors if dynamic allocation is enabled
`spark.dynamicAllocation.maxExecutors = infinity` - Upper bound for the number of executors if dynamic allocation is enabled

### Speculative Execution

MapReduce was designed to run on clusters of commodity hardware
Stragglers

**Configuring speculative execution:**
`spark.speculation = false` - If set to "true", performs speculative execution of tasks
`spark.speculation.interval = 100ms`
`spark.speculation.multiplier = 1.5`
`spark.speculation.quantile = 0.75` - fraction of tasks which must be complete before speculation is enabled for a particular stage

Example:
100 tasks, medium execution time = 120s
75 tasks should finish (100 * 0.75)
straggler > 180s (120 * 1.5)
`spark.speculation.multiplier = 1.5`
`spark.speculation.quantile = 0.75`

* Spark (as Hadoop) was built to run on commodity hardware
* Equal load, unequal resources cause stragglers
* Speculative execution may help
* Skew data causes false stragglers

